{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Ensure the output directories exist\n",
    "output_dir = 'output'\n",
    "checkpoint_dir = 'checkpoint'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Note\n",
    "# ld & lg = 0.003 are very blur\n",
    "# ld & lg = 0.0003 are blur\n",
    "\n",
    "# Parameters\n",
    "epochs = 300\n",
    "batch_size = 64\n",
    "learning_rate_d = 0.0004\n",
    "\n",
    "learning_rate_g = 0.004\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "latent_dim = 100\n",
    "img_size = 64  # Adjust based on your dataset\n",
    "channels = 3  # Adjust based on your dataset\n",
    "\n",
    "img_shape = (channels, img_size, img_size)\n",
    "\n",
    "# Check CUDA's presence\n",
    "cuda_is_present = True if torch.cuda.is_available() else False\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        def layer_block(input_size, output_size, normalize=True):\n",
    "            layers = [nn.Linear(input_size, output_size)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(output_size, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *layer_block(latent_dim, 128, normalize=False),\n",
    "            *layer_block(128, 256),\n",
    "            *layer_block(256, 512),\n",
    "            *layer_block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        verdict = self.model(img_flat)\n",
    "        return verdict\n",
    "\n",
    "# Initialize models and loss function\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "if cuda_is_present:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Custom dataset for cartoon images\n",
    "class CartoonDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.file_paths[idx])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load cartoon dataset\n",
    "dataset_path = '/Users/chutszkan/Desktop/Files/HKU/COMP7502/Test/datasets/simpsons_dataset_archive/simpsons_dataset/homer_simpson/'\n",
    "data_loader = DataLoader(\n",
    "    CartoonDataset(folder_path=dataset_path, transform=transform),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda_is_present else torch.FloatTensor\n",
    "\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=learning_rate_g, betas=(b1, b2))\n",
    "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=learning_rate_d, betas=(b1, b2))\n",
    "\n",
    "losses = []\n",
    "images_for_gif = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for i, images in enumerate(data_loader):\n",
    "        real_images = Variable(images.type(Tensor))\n",
    "        real_output = Variable(Tensor(images.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake_output = Variable(Tensor(images.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Training Generator\n",
    "        optimizer_generator.zero_grad()\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (images.shape[0], latent_dim))))\n",
    "        generated_images = generator(z)\n",
    "        generator_loss = adversarial_loss(discriminator(generated_images), real_output)\n",
    "        generator_loss.backward()\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # Training Discriminator\n",
    "        optimizer_discriminator.zero_grad()\n",
    "        discriminator_loss_real = adversarial_loss(discriminator(real_images), real_output)\n",
    "        discriminator_loss_fake = adversarial_loss(discriminator(generated_images.detach()), fake_output)\n",
    "        discriminator_loss = (discriminator_loss_real + discriminator_loss_fake) / 2\n",
    "        discriminator_loss.backward()\n",
    "        optimizer_discriminator.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch:=4d}/{epochs}] [Batch {i:=4d}/{len(data_loader)}] ---> \"\n",
    "              f\"[D Loss: {discriminator_loss.item():.6f}] [G Loss: {generator_loss.item():.6f}]\")\n",
    "\n",
    "    losses.append((generator_loss.item(), discriminator_loss.item()))\n",
    "    if epoch % 10 == 0:\n",
    "        image_filename = f'{output_dir}/images/epoch_{epoch}.png'\n",
    "        os.makedirs(f'{output_dir}/images', exist_ok=True)\n",
    "        save_image(generated_images.data[:25], image_filename, nrow=5, normalize=True)\n",
    "        images_for_gif.append(imageio.imread(image_filename))\n",
    "        # Save model checkpoints\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'generator_epoch_{epoch}.pth')\n",
    "        torch.save(generator.state_dict(), checkpoint_path)\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'discriminator_epoch_{epoch}.pth')\n",
    "        torch.save(discriminator.state_dict(), checkpoint_path)\n",
    "\n",
    "# Visualizing the losses at every epoch\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Generator')\n",
    "plt.plot(losses.T[1], label='Discriminator')\n",
    "plt.title(\"Training Losses\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{output_dir}/loss_plot.png')\n",
    "\n",
    "# Creating a gif of generated images at every epoch\n",
    "imageio.mimwrite(f'{output_dir}/generated_images.gif', images_for_gif, fps=len(images_for_gif)/5)\n",
    "\n",
    "# Load the final trained models for generating images\n",
    "generator.load_state_dict(torch.load(os.path.join(checkpoint_dir, f'generator_epoch_{epochs}.pth')))\n",
    "generator.eval()\n",
    "\n",
    "# Generate some images\n",
    "num_images = 10\n",
    "z = Variable(Tensor(np.random.normal(0, 1, (num_images, latent_dim))))\n",
    "generated_images = generator(z)\n",
    "\n",
    "# Save generated images\n",
    "generated_image_filename = f'{output_dir}/final_generated_images.png'\n",
    "save_image(generated_images.data, generated_image_filename, nrow=num_images, normalize=True)\n",
    "print(f'Generated images saved to {generated_image_filename}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
